<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>AI: Contemporary uses and questions for Audiovisual Entertainment. - Phileas Dazeley-Gaist</title><meta name="description" content="This short text discusses contemporary applications for AI (artificial intelligence) machine learning&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><script type="text/javascript" async src="https://www.googletagmanager.com/gtag/js?id=G-K13VH578CQ"></script><script type="text/javascript">window.dataLayer = window.dataLayer || [];
				  function gtag(){dataLayer.push(arguments);}
				  gtag('js', new Date());
				  gtag('config', 'G-K13VH578CQ' );</script><link rel="canonical" href="https://phileasdg.github.io/phileas-dazeley-gaist/ai-contemporary-uses-and-questions-for-audiovisual-entertainment/"><link rel="alternate" type="application/atom+xml" href="https://phileasdg.github.io/phileas-dazeley-gaist/feed.xml"><link rel="alternate" type="application/json" href="https://phileasdg.github.io/phileas-dazeley-gaist/feed.json"><meta property="og:title" content="AI: Contemporary uses and questions for Audiovisual Entertainment."><meta property="og:image" content="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/GAN-video-2.png"><meta property="og:image:width" content="1364"><meta property="og:image:height" content="510"><meta property="og:site_name" content="Phileas Dazeley-Gaist"><meta property="og:description" content="This short text discusses contemporary applications for AI (artificial intelligence) machine learning&hellip;"><meta property="og:url" content="https://phileasdg.github.io/phileas-dazeley-gaist/ai-contemporary-uses-and-questions-for-audiovisual-entertainment/"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@phileasdg"><meta name="twitter:title" content="AI: Contemporary uses and questions for Audiovisual Entertainment."><meta name="twitter:description" content="This short text discusses contemporary applications for AI (artificial intelligence) machine learning&hellip;"><meta name="twitter:image" content="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/GAN-video-2.png"><link rel="stylesheet" href="https://phileasdg.github.io/phileas-dazeley-gaist/assets/css/style.css?v=ff30ce3ac4c3be676e9c9a0dc5db8575"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://phileasdg.github.io/phileas-dazeley-gaist/ai-contemporary-uses-and-questions-for-audiovisual-entertainment/"},"headline":"AI: Contemporary uses and questions for Audiovisual Entertainment.","datePublished":"2022-08-12T08:14","dateModified":"2022-08-12T08:14","image":{"@type":"ImageObject","url":"https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/GAN-video-2.png","height":510,"width":1364},"description":"This short text discusses contemporary applications for AI (artificial intelligence) machine learning&hellip;","author":{"@type":"Person","name":"Phileas Dazeley-Gaist","url":"https://phileasdg.github.io/phileas-dazeley-gaist/authors/phileas-dazeley-gaist/"},"publisher":{"@type":"Organization","name":"Phileas Dazeley-Gaist"}}</script><a hidden rel="me" href="https://mathstodon.xyz/@phileasdg">Mastodon</a></head><body><header class="header" id="js-header"><a href="https://phileasdg.github.io/phileas-dazeley-gaist/" class="logo">Phileas Dazeley-Gaist</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://phileasdg.github.io/phileas-dazeley-gaist/" target="_self">Home</a></li><li><a href="https://phileasdg.github.io/phileas-dazeley-gaist/a-few-words-about-me/" target="_self">About</a></li><li><a href="https://phileasdg.github.io/phileas-dazeley-gaist/resume-cv/" target="_self">Resume / CV</a></li><li><a href="https://phileasdg.github.io/phileas-dazeley-gaist/inquiries/" target="_self">Inquiries</a></li></ul></nav></header><main><div class="wrapper"><article class="post"><header class="post__header"><a href="https://phileasdg.github.io/phileas-dazeley-gaist/tags/undergraduate-work/" class="post__maintag">Undergraduate work</a><h1 class="post__title">AI: Contemporary uses and questions for Audiovisual Entertainment.</h1><div class="post__meta"><div class="post__author">By <a href="https://phileasdg.github.io/phileas-dazeley-gaist/authors/phileas-dazeley-gaist/" class="invert" rel="author" title="Phileas Dazeley-Gaist">Phileas Dazeley-Gaist</a></div><time datetime="2022-08-12T08:14">August 12, 2022</time></div></header><figure class="post__featured-image"><img src="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/GAN-video-2.png" srcset="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-video-2-xs.png 300w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-video-2-sm.png 480w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-video-2-md.png 768w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-video-2-lg.png 1200w" sizes="(min-width: 56.25em) 100vw, (min-width: 37.5em) 50vw, 100vw" loading="eager" height="510" width="1364" alt=""></figure><div class="post__inner"><div class="post__entry"><div class="page" title="Page 1"><div class="layoutArea"><div class="column"><p>This short text discusses contemporary applications for AI (artificial intelligence) machine learning agents as well as AI agent behaviour in the production, distribution, and consumption of audiovisual media. In this context, it aims to raise questions about the implications of AI based task completion. </p><p>Note: I wrote this text for a course in Film Theory at College of the Atlantic taught by Professor Colin Capers in Spring 2020. A lot has changed in the field of AI since then, and although there now exist many new and more powerful approaches to using AI in Audiovisual Entertainment and New Media, these recent developments are not reflected in this short survey-essay. </p><h2>Introduction</h2><div class="page" title="Page 1"><div class="layoutArea"><div class="column"><p>In 2012, Google Brain researchers at Google’s semi-secret research laboratory, Google X successfully trained an artificial neural network to recognise cat images from a vast dataset of YouTube videos (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">John Markoff, ‘How Many Computers to Identify a Cat? 16,000’, The New York Times, 25 June 2012, sec. Technology, <a href="https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of- machine-learning.html">link 1</a></span>). <span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">At the time, this milestone in machine learning garnered wide mediatic coverage, notably from the New York Times and The Verge (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Ibid; Liat Clark, ‘Google’s Artificial Brain Learns to Find Cat Videos’, Wired, 26 June 2012, <a href="https://www.wired.com/2012/06/google-x-neural-network/">link 2</a></span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Today, eight years later, image recognition technology is cheaper, more versatile and widely available than ever, in fact, it is now a standard feature on most modern mobile phones, digital cameras, and laptops at no specified cost to the end-user.</span></p><div class="page" title="Page 1"><div class="layoutArea"><div class="column"><p>Image recognition is just one of the many implementations of AI systems in modern consumer electronics. We interact with AI constantly through electronic devices, knowingly and unknowingly as we go about our day-to-day lives. AIs process the images on our <span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">smartphone cameras (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Sam Byford, ‘How AI Is Changing Photography’, The Verge, 31 January 2019, <a href="https://www.theverge.com/2019/1/31/18203363/ai-artificial-intelligence-photography-google-photos-apple- huawei">link 3</a></span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">), </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">model our tastes and consumer behaviours to better market products to us online (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Charles Taylor, ‘Can Artificial Intelligence Eliminate Consumer Privacy Concerns For Digital Advertisers?’, Forbes, accessed 4 June 2020, <a href="https://www.forbes.com/sites/charlesrtaylor/2019/07/29/can-artificial- intelligence-eliminate-consumer-privacy-concerns-for-digital-advertisers/">link 4</a></span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">), </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">determine what media to recommend to us (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Joss Fong, ‘How Smart Is Today’s Artificial Intelligence?’, Vox, 19 December 2017, <a href="https://www.vox.com/videos/2017/12/19/16792294/artificial-intelligence-limits-of-ai">link 5</a></span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">), </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">and fulfil many other tasks that simultaneously serve us and shape our behaviours.</span></p><div class="page" title="Page 2"><div class="layoutArea"><div class="column"><p>The ever-increasing ubiquity of AI agents in our lives echoes the huge advancements in AI research and electronic hardware in the last decade. Most relevant to this exploration are advancements and discoveries in the field of machine learning that have led to significant changes in the way we make, consume, and think about audiovisual media. AI involvement in the production, distribution, and consumption (referred to from here on as PDC) of audiovisual media is growing as businesses, researchers, and enthusiasts train neural networks and provide them with appropriate interfaces to tackle PDC tasks. Provided it has access to the right interface, there exists an AI agent capable of carrying out every audiovisual media PDC task.</p><h2>Existing AI implementations for audiovisual media PDC</h2><p>While AI agents can be trained to perform any audiovisual media PDC task, the properties of the agent’s work output might differ widely from the output of a similarly instructed human worker. Generally, AI agent capabilities are measured by comparing the agent’s performance to a set of human-designed criteria. Against these criteria, AI’s fare very differently depending on their design and task. While some do very well, even occasionally <span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">surpassing human abilities altogether, others produce poor results. For the purposes of this text, AI agent performance is secondary to AI agent behaviour.</span></p><div class="page" title="Page 3"><div class="layoutArea"><div class="column"><p>The following three sections of this text describe some notable examples of AI implementations in the production, distribution, and consumption of audiovisual content, supplemented with short descriptions and evaluations of their conduct and work output.</p><h3>Audiovisual media production</h3><h4>1) Benjamin: screenwriting, video editing and visual effects</h4><p>LSTM (long short-term memory), also known as Benjamin is an artificial recurrent neural network (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Annalee Newitz, ‘Movie Written by Algorithm Turns out to Be Hilarious and Intense’, Ars Technica, 6 September 2016, <a href="https://arstechnica.com/gaming/2016/06/an-ai-wrote-this-movie-and-its-strangely-moving/">link 6</a></span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">) </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">partially credited for producing three short films: Sunspring (2016, starring Thomas Middleditch), It’s no game (2017, starring David Hasselhoff), and Zone Out (2018, starring Thomas Middleditch) (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">‘Films by Benjamin the A.I.’, Therefore Films, accessed 5 June 2020, <a href="http://www.thereforefilms.com/films- by-benjamin-the-ai.html">link 7</a></span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Benjamin is credited for generating the Sunspring screenplay (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Newitz, ‘Movie Written by Algorithm Turns out to Be Hilarious and Intense’)</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">, co-authoring the screenplay of It’s no game (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Annalee Newitz, ‘An AI Wrote All of David Hasselhoff’s Lines in This Bizarre Short Film’, Ars Technica, 25 April 2017, <a href="https://arstechnica.com/gaming/2017/04/an-ai-wrote-all-of-david-hasselhoffs-lines-in-this- demented-short-film/">link 8</a>), </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">and for writing and editing Zone Out (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">‘AI Made a Movie With a “Silicon Valley” Star—and the Results Are Horrifyingly Encouraging’, Wired, accessed 5 June 2020, <a href="https://www.wired.com/story/ai-filmmaker-zone-out/">link 9</a>). </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Benjamin’s films are freely available for viewing online at <a href="www.thereforefilms.com">thereforefilms.com</a>.</span></p><div class="page" title="Page 3"><div class="layoutArea"><div class="column"><p>For each film, the AI agent was trained on different datasets:</p><ul><li>To write Sunspring, writer, artist and data scientist Ross Goodwin fed it dozens (a small number for an AI dataset) of sci-fi movie scripts, resulting in absurd and ominous sci-fi-esque dialogues (Newitz, ‘Movie Written by Algorithm Turns out to Be Hilarious and Intense’.)</li><li>For It’s no game, Goodwin had Benjamin study David Hasselhoff shows to produce Hasselhoff’s lines. While the result is nonsensical, Hasselhoff seemed genuinely moved by the script. In an article by Annalee Newitz on Ars Technica, Hasselhoff reflects: “This AI really had a handle on what's going on in my life and it was strangely emotional” (Ibid).</li><li>To produce Zone Out, Goodwin trained Benjamin from a pool of films in the public domain (‘AI Made a Movie With a “Silicon Valley” Star—and the Results Are Horrifyingly Encouraging’<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). </span></span>The result is a semi-coherent absurdist black and white drama with actors deepfaked over shots from traditionally produced cinematic content (Deepfakes are an AI based video manipulation technique which maps faces from a reference database onto an image sequence<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">).</span></li></ul><div class="page" title="Page 4"><div class="layoutArea"><div class="column"><p>While Benjamin’s writing and editing often result in absurd dialogue and stories, its work demonstrates clear pattern recognition vis-à-vis oral and visual language in Film.</p><h4>2) Watson: film trailer video editing</h4><p>In 2016, 20th Century Fox partnered with IBM to produce the official film trailer for Morgan (2016) using Watson, IBM’s proprietary AI business platform (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">‘IBM Research Takes Watson to Hollywood with the First “Cognitive Movie Trailer”’, THINK Blog, 31 August 2016, </span><a href="https://www.ibm.com/blogs/think/2016/08/cognitive-movie-trailer/">link 10</a><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">)</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">. </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">The resulting trailer is blandly edited and generally unexciting, but it is also difficult to distinguish from many (presumably) human-directed trailers. The film flopped, and critics and audiences alike widely dismissed the film on its release.</span></p><div class="page" title="Page 4"><div class="layoutArea"><div class="column"><h4>3) Stanford &amp; Adobe research: Computational Video Editing for Dialogue-Driven Scenes</h4><p>Computational Video Editing for Dialogue-Driven Scenes is a research paper co-authored by Mackenzie Leake, Abe Davis, Anh Truong, and Maneesh Agrawala (of Stanford University and Adobe Research). The article was published in July 2017 in the journal ACM Transactions on Graphics. It presents an AI-driven system for efficiently editing video of <span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">dialogue-driven scenes. The system employs film editing idioms to instruct the AI agent to edit according to defined parameters with remarkable success. The results, with regards to their respect of editing idioms, could easily be mistaken to have been edited by an experienced human video-editor.</span></p></div></div></div></div></div></div><div class="page" title="Page 5"><div class="layoutArea"><div class="column"><h4>4) GANs: image manipulation and image sequence generation</h4><p>Generative Adversarial Networks (usually abbreviated to GANs) are machine learning frameworks which employ two AI networks (often the same agent twice) competing to improve an AI’s work output for a given task (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Generative Adversarial Networks (GANs) - Computerphile, accessed 5 June 2020, <a href="https://www.youtube.com/watch?v=Sw9r8CL98N0">link 11</a></span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">In this competition, one network acts as a generator, outputting information, while the other acts as a discriminator, evaluating the work submitted by the generator. The discriminator’s evaluation is then fed back into the generator as a score, allowing the generator to adjust its output and improve its work (Ibid).</span></p><div class="page" title="Page 5"><div class="layoutArea"><div class="column"><p>GANs have shown great capacity to manipulate and generate new images and image sequences from described attributes. Manipulating Attributes of Natural Scenes via Hallucination, a research paper published in February 2020 in the journal ACM Transactions on Graphics and co-authored by Levent Karacan, Zeynep Akata, Aykut Erdem, and Erkut Erdem explores the generation and manipulation of nature scenes through a process of neural network image hallucination (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Levent Karacan et al., ‘Manipulating Attributes of Natural Scenes via Hallucination’, ACM Transactions on Graphics 39, no. 1 (11 February 2020): 1–17, </span><a href="https://doi.org/10.1145/3368312">link 12</a><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">The results (example image below) are astonishing.</span></p><figure class="post__image"><img loading="lazy" src="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/GAN.png" alt="" width="1412" height="528" sizes="(max-width: 48em) 100vw, 100vw" srcset="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-xs.png 300w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-sm.png 480w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-md.png 768w"><figcaption><div class="page" title="Page 6"><div class="layoutArea"><div class="column"><p>GAN image manipulation example (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Levent Karacan et al., ‘Manipulating Attributes of Natural Scenes via Hallucination’, ACM Transactions on Graphics 39, no. 1 (11 February 2020): 1–17, </span><a href="https://doi.org/10.1145/3368312" style="font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">https://doi.org/10.1145/3368312</a><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">)</span></p></div></div></div></figcaption></figure><div class="page" title="Page 6"><div class="layoutArea"><div class="column"><p>Another paper, Adversarial Video Generation on Complex Datasets, published by Google DeepMind researchers Aidan Clark, Jeff Donahue, and Karen Simonyan in 2019, demonstrates excellent potential for GAN based video generation. In this paper, through the GAN framework, researchers successfully trained an AI agent to produce 256x256 pixel resolution videos of a duration of 48 frames (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Aidan Clark, Jeff Donahue, and Karen Simonyan, ‘Adversarial Video Generation on Complex Datasets’, 15 July 2019, </span><a href="https://arxiv.org/abs/1907.06571v2">link 13</a><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). Their results (example image below) are promising.</span></p><figure class="post__image"><img loading="lazy" src="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/GAN-video.png" alt="" width="1364" height="510" sizes="(max-width: 48em) 100vw, 100vw" srcset="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-video-xs.png 300w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-video-sm.png 480w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/12/responsive/GAN-video-md.png 768w"><figcaption>GAN video generation example (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Aidan Clark, Jeff Donahue, and Karen Simonyan, ‘Adversarial Video Generation on Complex Datasets’, 15 July 2019, </span><a href="https://arxiv.org/abs/1907.06571v2" style="font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">https://arxiv.org/abs/1907.06571v2</a><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">)</span></figcaption></figure><div class="page" title="Page 7"><div class="layoutArea"><div class="column"><h4>Tacotron: voice modelling and speech synthesis</h4><div class="page" title="Page 7"><div class="layoutArea"><div class="column"><p>In Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions, a computation and language paper published by researchers at Google and the University of California, Berkeley first published in December 2017 researchers describe Tacotron 2: “a neural network architecture for speech synthesis directly from text” (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Jonathan Shen et al., ‘Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions’, 16 December 2017, <a href="https://arxiv.org/abs/1712.05884v2">link 14</a></span>). <span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">In a post to the Google AI Blog (ai.googleblog.com) from around the same time, software engineers Jonathan Shen and Ruoming Pang summarise the systems’ ability to generate human-like speech from text using embedded voice models (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">‘Tacotron 2: Generating Human-like Speech from Text’, Google AI Blog (blog), accessed 5 June 2020, </span><a href="http://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html">link 15</a><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">The results are often incredibly impressive, although the system has difficulty accurately synthesising words where the tonal accent position is ambiguous (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Shen et al., ‘Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions’</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">).</span></p><figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/ZPeu-ByEi0s?feature=oembed" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure><div class="page" title="Page 8"><div class="layoutArea"><div class="column"><h4>MuseNet &amp; FlowMachines: music generation</h4><div class="page" title="Page 8"><div class="layoutArea"><div class="column"><p>MuseNet is a deep neural network developed by OpenAI that can generate four-minute-long musical compositions for a selection of ten instruments. By training the system on hundreds of thousands of MIDI files, researchers have taught the network to recognise different patterns of musical styles, harmony, and rhythms (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">‘MuseNet’, OpenAI, 25 April 2019, </span><a href="https://openai.com/blog/musenet/">link 16</a><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">MuseNet can generate music from a reference, from style, or rhythm prompts, or without any input whatsoever (Ibid). Its output could easily be mistaken for human work. MuseNet music samples are available on the OpenAI website, openai.com.</span></p><div class="page" title="Page 8"><div class="layoutArea"><div class="column"><p>Much like MuseNet, FlowMachines is a music generation neural network. Developed by scientists at Sony CSL Paris and Pierre and Marie Curie University (UPMC), the program garnered much attention in 2016 for composing two music pieces now freely available to listen to on YouTube (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Mr Shadow: A Song Composed by Artificial Intelligence, accessed 5 June 2020, </span><a href="https://www.youtube.com/watch?v=lcGYEXJqun8">link 17</a><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">FlowMachines creates music much more complex than MuseNet, but its music generation is more recognisably machine-like than its OpenAI counterpart (example song linked below).</span></p><figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/lcGYEXJqun8?feature=oembed" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure><div class="page" title="Page 9"><div class="layoutArea"><div class="column"><h3>Audiovisual media distribution:</h3><div class="page" title="Page 9"><div class="layoutArea"><div class="column"><h4>Targeted recommendations and social network advertising</h4><div class="page" title="Page 9"><div class="layoutArea"><div class="column"><p>Many online music and video streaming services use machine learning to generate media recommendations for clients algorithmically. Among others, this is the case for Netflix, whose recommendation system analyses viewer data in combination with the characteristics of viewed content (storyline, character makeup, genre, mood, etc.) (<span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Libby Plummer, ‘This Is How Netflix’s Top-Secret Recommendation System Works’, Wired UK, 22 August 2017, </span><a href="https://www.wired.co.uk/article/how-do-netflixs-algorithms-work-machine-learning-helps-to-predict-what-viewers-will-like">link 18</a><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">). </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Similarly, most social media platforms farm user personal information to generate elaborate consumer profiles through big data, allowing them to better target users through advertisements (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Kalev Leetaru, ‘Social Media Companies Collect So Much Data Even They Can’t Remember All The Ways They Surveil Us’, Forbes, accessed 5 June 2020, <a href="https://www.forbes.com/sites/kalevleetaru/2018/10/25/social-media-companies-collect-so-much-data-even-they-cant-remember-all-the-ways-they-surveil-us/">link 19</a></span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">).</span></p><div class="page" title="Page 9"><div class="layoutArea"><div class="column"><h3>Audiovisual media consumption</h3><div class="page" title="Page 9"><div class="layoutArea"><div class="column"><p>AI agents that produce images and sound learn through datasets of the same audio and visual material. This applies to all AI agent examples described under the Audiovisual media production section of this paper. Artificial neural networks far outpace humans at consuming audiovisual content, but the implications of content reading are different for human and AIs. While AI’s may be able to process content much faster and more efficiently than human beings, they do not currently possess any means to understand the content that they consume. Nonetheless, AI systems often surpass humans at extracting specific information from content.</p><div class="page" title="Page 9"><div class="layoutArea"><div class="column"><h4>Watch, Listen, Attend and Spell: AI audiovisual media information extraction</h4><div class="page" title="Page 9"><div class="layoutArea"><div class="column"><p>Watch, Listen, Attend and Spell (abbreviated as WLAS), released in 2016 is a machine learning algorithm developed by researches at Google DeepMind to lip-read human beings on video. The program, trained on news footage from the BBC, can read and caption video <span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">content containing a visible speaker with 46.8 per cent accuracy, 34.4 per cent higher than human professionals (</span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">James Vincent, ‘Google’s AI Can Now Lip Read Better than Humans after Watching Thousands of Hours of TV’, The Verge, 24 November 2016, <a href="https://www.theverge.com/2016/11/24/13740798/google-deepmind-ai-lip-reading-tv">link 20</a></span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">).</span></p><div class="page" title="Page 10"><div class="layoutArea"><div class="column"><h2>Concluding statement</h2><p>What is a human task? Historically, what has fallen within the umbrella are tasks that human beings could not delegate to other animals or machines. As AI agents continue to improve, how will our current understanding of what is or is not a human task change? As AI agents develop, the roles they play in our day-to-day lives will continue to expand. Which tasks will AIs assist in, and which will they take over? It is still too early to tell. Despite the impressive abilities of some AI agents, it seems unlikely that generalised AI involvement in audiovisual media PDC is around the corner. Perhaps it is, but then again, probably not.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><p> </p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><footer><p class="post__last-updated">This article was updated on August 12, 2022</p><div class="post__tags-share"><ul class="post__tag"><li><a href="https://phileasdg.github.io/phileas-dazeley-gaist/tags/undergraduate-work/">Undergraduate work</a></li></ul><aside class="post__share"></aside></div><div class="post__bio author"><div><h3 class="h6 author__name"><a href="https://phileasdg.github.io/phileas-dazeley-gaist/authors/phileas-dazeley-gaist/" class="invert" title="Phileas Dazeley-Gaist">Phileas Dazeley-Gaist</a></h3></div></div><nav class="post__nav"><div class="post__nav__next"><a class="post__nav__link" href="https://phileasdg.github.io/phileas-dazeley-gaist/graphical-iteration-in-r/" rel="prev">Next Post<h3 class="h6">Graphical iteration in R</h3></a></div></nav></footer></div></article></div><div class="post__related"><div class="wrapper"><h2 class="h5">Related posts</h2><div class="l-grid l-grid--4"><article class="c-card"><a href="https://phileasdg.github.io/phileas-dazeley-gaist/economic-uses-of-selected-species-in-the-family-lamiaceae/" class="c-card__image"><img src="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/27/De_Historia_stirpium_commentarii_insignes_...Fuchs_Leonhart_bpt6k1511104z_510.jpg" srcset="https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/27/responsive/De_Historia_stirpium_commentarii_insignes_...Fuchs_Leonhart_bpt6k1511104z_510-xs.jpg 300w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/27/responsive/De_Historia_stirpium_commentarii_insignes_...Fuchs_Leonhart_bpt6k1511104z_510-sm.jpg 480w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/27/responsive/De_Historia_stirpium_commentarii_insignes_...Fuchs_Leonhart_bpt6k1511104z_510-md.jpg 768w, https://phileasdg.github.io/phileas-dazeley-gaist/media/posts/27/responsive/De_Historia_stirpium_commentarii_insignes_...Fuchs_Leonhart_bpt6k1511104z_510-lg.jpg 1200w" sizes="(min-width: 56.25em) 100vw, (min-width: 37.5em) 50vw, 100vw" loading="lazy" height="1114" width="2558" alt=""></a><div class="c-card__wrapper"><header class="c-card__header"><div class="c-card__tag"><a href="https://phileasdg.github.io/phileas-dazeley-gaist/tags/undergraduate-work/">Undergraduate work</a></div><h3 class="c-card__title"><a href="https://phileasdg.github.io/phileas-dazeley-gaist/economic-uses-of-selected-species-in-the-family-lamiaceae/" class="invert">Economic Uses of Selected Species in the Family Lamiaceae</a></h3></header><footer class="c-card__meta"><time datetime="2023-03-10T19:19">March 10, 2023</time></footer></div></article></div></div></div></main><footer class="footer"><div class="footer__social"><a href="https://www.instagram.com/phileasdg/" aria-label="Instagram" class="instagram"><svg><use xlink:href="https://phileasdg.github.io/phileas-dazeley-gaist/assets/svg/svg-map.svg#instagram"/></svg> </a><a href="https://www.linkedin.com/in/phileas/" aria-label="LinkedIn" class="linkedin"><svg><use xlink:href="https://phileasdg.github.io/phileas-dazeley-gaist/assets/svg/svg-map.svg#linkedin"/></svg> </a><a href="https://www.youtube.com/@phileasdg" aria-label="Youtube" class="youtube"><svg><use xlink:href="https://phileasdg.github.io/phileas-dazeley-gaist/assets/svg/svg-map.svg#youtube"/></svg></a></div><div class="footer__copyright">Phileas Dazeley-Gaist</div></footer><script>window.publiiThemeMenuConfig = {    
      mobileMenuMode: 'sidebar',
      animationSpeed: 300,
      submenuWidth: 'auto',
      doubleClickTime: 500,
      mobileMenuExpandableSubmenus: true, 
      relatedContainerForOverlayMenuSelector: '.navbar',
   };</script><script defer="defer" src="https://phileasdg.github.io/phileas-dazeley-gaist/assets/js/scripts.min.js?v=66112a161d5939d966ce2b20c13988d9"></script><script>var images = document.querySelectorAll('img[loading]');

      for (var i = 0; i < images.length; i++) {
         if (images[i].complete) {
               images[i].classList.add('is-loaded');
         } else {
               images[i].addEventListener('load', function () {
                  this.classList.add('is-loaded');
               }, false);
         }
      }</script></body></html>